name: first test

on: 
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
    types: [ opened ]
env:
  ACTIONS_STEP_DEBUG: "true"
  ACTIONS_RUNNER_DEBUG: "true"

jobs:
  test:
    runs-on: p800_82
    timeout-minutes: 60
    #: iregistry.baidu-int.com/xmlir/xmlir_ubuntu_2004_x86_64:v0.37_base
    steps:
      - name: Checkout
        uses: actions/checkout@v6
      # - name: Install dependencies
      #   run: pip install pytest
      - name: check env
        run: |
          set -euxo pipefail
          cat /etc/os-release
          pwd
          echo $PATH 
          source ~/.bashrc
          python --version
          
      # - name: Test
      #   env:  
      #       XPU_VISIBLE_DEVICES: "0"
      #       XFT_USE_FAST_SWIGLU: "1"
      #       XPU_USE_FAST_SWIGLU: "1"
      #       XMLIR_CUDNN_ENABLED: "1"
      #       XPU_USE_DEFAULT_CTX: "1"
      #       XMLIR_FORCE_USE_XPU_GRAPH: "1"
      #       XPU_USE_MOE_SORTED_THRES: "128"
      #       VLLM_USE_V1: "1"
      #       XMLIR_ENABLE_MOCK_TORCH_COMPILE: "false"
      #       USE_ORI_ROPE: "1"
      #   run: |
      #     export VLLM_HOST_IP=$(hostname -i)
      #     unset XPU_DUMMY_EVENT
      #     python -m vllm.entrypoints.openai.api_server       --host 0.0.0.0       --port 8356       --model /home/hongweijie/Qwen3-0.6B       --gpu-memory-utilization 0.9       --trust-remote-code       --max-model-len 32768       --tensor-parallel-size 1       --dtype float16       --max_num_seqs 128       --max_num_batched_tokens 32768       --block-size 128       --no-enable-prefix-caching       --no-enable-chunked-prefill       --distributed-executor-backend mp       --served-model-name Qwen3-VL-30B-A3B-Instruct       --compilation-config '{"splitting_ops": ["vllm.unified_attention", 
      #                                           "vllm.unified_attention_with_output",
      #                                           "vllm.unified_attention_with_output_kunlun",
      #                                           "vllm.mamba_mixer2", 
      #                                           "vllm.mamba_mixer", 
      #                                           "vllm.short_conv", 
      #                                           "vllm.linear_attention", 
      #                                           "vllm.plamo2_mamba_mixer", 
      #                                           "vllm.gdn_attention", 
      #                                           "vllm.sparse_attn_indexer"]}'
      